<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Introduction · Spark</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link href="assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>Spark</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" action="search.html"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li class="current"><a class="toctext" href="index.html">Introduction</a><ul class="internal"><li><a class="toctext" href="#Overview-1">Overview</a></li><li><a class="toctext" href="#RDD-Interface-1">RDD Interface</a></li><li><a class="toctext" href="#SQL-Interface-1">SQL Interface</a></li><li><a class="toctext" href="#Current-Limitations-1">Current Limitations</a></li><li><a class="toctext" href="#Trademarks-1">Trademarks</a></li></ul></li><li><a class="toctext" href="api.html">API Reference</a></li></ul></nav><article id="docs"><header><nav><ul><li><a href="index.html">Introduction</a></li></ul><a class="edit-page" href="https://github.com/dfdx/Spark.jl/tree/66cd7e089074d56c34d26fe66d6d896ce92d783e/index.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Introduction</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Introduction-1" href="#Introduction-1">Introduction</a></h1><h2><a class="nav-anchor" id="Overview-1" href="#Overview-1">Overview</a></h2><p>Spark.jl is the package that allows the execution of Julia programs on the Apache Spark™ platform. It supports running pure Julia scripts on Julia data structures, while utilising the data and code distribution capabalities of Apache Spark. It supports multiple cluster types (in client mode), and can be consider as an analogue to PySpark or RSpark within the Julia ecosystem. It supports running within on-premise installations, as well as hosted instance such as Amazon EMR and Azure HDInsight. </p><h3><a class="nav-anchor" id="Installation-1" href="#Installation-1">Installation</a></h3><p>Spark.jl requires at least Java 7 and Maven to be installed and available in PATH. </p><pre><code class="language-none">Pkg.add(&quot;Spark.jl&quot;)</code></pre><h3><a class="nav-anchor" id="Basic-Usage-1" href="#Basic-Usage-1">Basic Usage</a></h3><p>The <code>Spark.init()</code> method must to called at the beginning of a session to initialise the JVM. Subsequently a <code>SparkContext</code> is created to serve as the primary reference to a Spark instance.  </p><pre><code class="language-none">using Spark
Spark.init()
sc = SparkContext(master=&quot;local&quot;)</code></pre><h3><a class="nav-anchor" id="Cluster-Tyes-1" href="#Cluster-Tyes-1">Cluster Tyes</a></h3><p>This package supports multiple cluster types (in client mode): <code>local</code>, <code>standalone</code>, <code>mesos</code> and <code>yarn</code>. The location of the cluster (in case of mesos or standalone) or the cluster type (in case of local or yarn) must be passed as a parameter <code>master</code> when creating a Spark context. For YARN based clusters, the cluster parameters are picked up from <code>spark-defaults.conf</code>, which must be accessible via a <code>SPARK_HOME</code> environment variable. </p><h2><a class="nav-anchor" id="RDD-Interface-1" href="#RDD-Interface-1">RDD Interface</a></h2><p>The primary interface exposed vis this package is the Spark RDD object. RDD&#39;s may be created from any Julia iterator via the <code>parallelize</code> method. Alternatively, the <code>text_file</code> method may be used to read data from any Spark supported filesystem, such as <code>HDFS</code></p><p>Julia functions are passed as parameters to the various Spark operations. These functions must either be anonymous functions defined inline within the spark call, or they must be available on all nodes. Functions may be made available by installing Julia packages on all nodes, or via the <code>@attach</code> macro that will make any julia script file available on all the workder nodes. </p><h3><a class="nav-anchor" id="Example:-Count-lines-in-file-1" href="#Example:-Count-lines-in-file-1">Example: Count lines in file</a></h3><pre><code class="language-none">sc = SparkContext(master=&quot;local&quot;)
path = &quot;file:///var/log/syslog&quot;
txt = text_file(sc, path)
count(txt)
close(sc)</code></pre><h3><a class="nav-anchor" id="Example:-Map-/-Reduce-on-Standalone-master-1" href="#Example:-Map-/-Reduce-on-Standalone-master-1">Example: Map / Reduce on Standalone master</a></h3><pre><code class="language-none">sc = SparkContext(master=&quot;spark://spark-standalone:7077&quot;, appname=&quot;Say &#39;Hello!&#39;&quot;)
path = &quot;file:///var/log/syslog&quot;
txt = text_file(sc, path)
rdd = map(txt, line -&gt; length(split(line)))
reduce(rdd, +)
close(sc)</code></pre><h3><a class="nav-anchor" id="Example:-Map-partitions-on-Mesos-and-HDFS-1" href="#Example:-Map-partitions-on-Mesos-and-HDFS-1">Example: Map partitions on Mesos and HDFS</a></h3><pre><code class="language-none">sc = SparkContext(master=&quot;mesos://mesos-master:5050&quot;)
path = &quot;hdfs://namenode:8020/user/hdfs/test.log&quot;
txt = text_file(sc, path)
rdd = map_partitions(txt, it -&gt; filter(line -&gt; contains(line, &quot;a&quot;), it))
collect(rdd)
close(sc)</code></pre><h2><a class="nav-anchor" id="SQL-Interface-1" href="#SQL-Interface-1">SQL Interface</a></h2><p>A recent addition to this package is a DataFrame+SQL interface to Spark. In the examples below, it is assumed that you have a file people.json with content like this:</p><pre><code class="language-none">{&quot;name&quot;: &quot;Alice&quot;, &quot;age&quot;: 27}
{&quot;name&quot;: &quot;Bob&quot;, &quot;age&quot;: 32}</code></pre><h3><a class="nav-anchor" id="Example:-Read-dataframe-from-JSON-and-collect-to-a-driver-1" href="#Example:-Read-dataframe-from-JSON-and-collect-to-a-driver-1">Example: Read dataframe from JSON and collect to a driver</a></h3><pre><code class="language-none">spark = SparkSession()
df = read_json(spark, &quot;/path/to/people.json&quot;)
collect(df)</code></pre><h3><a class="nav-anchor" id="Example:-Read-JSON-and-write-Parquet-1" href="#Example:-Read-JSON-and-write-Parquet-1">Example: Read JSON and write Parquet</a></h3><pre><code class="language-none">spark = SparkSession()
df = read_json(spark, &quot;/path/to/people.json&quot;)
write_parquet(df, &quot;/path/to/people.parquet&quot;)</code></pre><h2><a class="nav-anchor" id="Current-Limitations-1" href="#Current-Limitations-1">Current Limitations</a></h2><ul><li><p>Jobs can be submitted from Julia process attached to the cluster in <code>client</code> deploy mode. <code>Cluster</code> mode is not fully supported, and it is uncertain if it is useful in the Julia context. </p></li><li><p>Since records are serialised between Java and Julia at the edges, the maximum size of a single row in an RDD is 2GB, due to Java array indices being limited to 32 bits. </p></li></ul><h2><a class="nav-anchor" id="Trademarks-1" href="#Trademarks-1">Trademarks</a></h2><p>Apache®, <a href="http://spark.apache.org">Apache Spark and Spark</a> are registered trademarks, or trademarks of the <a href="http://www.apache.org/">Apache Software Foundation</a> in the United States and/or other countries.</p><footer><hr/><a class="next" href="api.html"><span class="direction">Next</span><span class="title">API Reference</span></a></footer></article></body></html>
