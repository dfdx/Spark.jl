<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>SQL / DataFrames · Spark</title><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.044/juliamono.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.11/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">Spark</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Introduction</a></li><li class="is-active"><a class="tocitem" href>SQL / DataFrames</a><ul class="internal"><li><a class="tocitem" href="#DataFrame-Creation"><span>DataFrame Creation</span></a></li><li><a class="tocitem" href="#Viewing-Data"><span>Viewing Data</span></a></li><li><a class="tocitem" href="#Selecting-and-Accessing-Data"><span>Selecting and Accessing Data</span></a></li><li><a class="tocitem" href="#Grouping-Data"><span>Grouping Data</span></a></li><li><a class="tocitem" href="#Getting-Data-in/out"><span>Getting Data in/out</span></a></li><li><a class="tocitem" href="#Working-with-SQL"><span>Working with SQL</span></a></li></ul></li><li><a class="tocitem" href="../streaming/">Structured Streaming</a></li><li><a class="tocitem" href="../api/">API Reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>SQL / DataFrames</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>SQL / DataFrames</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/dfdx/Spark.jl/blob/master/docs/src/sql.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="SQL-/-DataFrames"><a class="docs-heading-anchor" href="#SQL-/-DataFrames">SQL / DataFrames</a><a id="SQL-/-DataFrames-1"></a><a class="docs-heading-anchor-permalink" href="#SQL-/-DataFrames" title="Permalink"></a></h1><p>This is a quick introduction into the Spark.jl core functions. It closely follows the official <a href="https://spark.apache.org/docs/latest/api/python/getting_started/quickstart_df.html">PySpark tutorial</a> and copies many examples verbatim. In most cases, PySpark docs should work for Spark.jl as is or with little adaptation.</p><p>Spark.jl applications usually start by creating a <code>SparkSession</code>:</p><pre><code class="language-julia hljs">using Spark

spark = SparkSession.builder.appName(&quot;Main&quot;).master(&quot;local&quot;).getOrCreate()</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">SparkSession()</code></pre><p>Note that here we use dot notation to chain function invocations. This makes the code more concise and also mimics Python API, making translation of examples easier. The same example could also be written as:</p><pre><code class="language-julia hljs">using Spark
import Spark: appName, master, getOrCreate

builder = SparkSession.builder
builder = appName(builder, &quot;Main&quot;)
builder = master(builder, &quot;local&quot;)
spark = getOrCreate(builder)</code></pre><p>See <a href="../api/#Spark.@chainable"><code>@chainable</code></a> for the details of the dot notation.</p><h2 id="DataFrame-Creation"><a class="docs-heading-anchor" href="#DataFrame-Creation">DataFrame Creation</a><a id="DataFrame-Creation-1"></a><a class="docs-heading-anchor-permalink" href="#DataFrame-Creation" title="Permalink"></a></h2><p>In simple cases, a Spark DataFrame can be created via <code>SparkSession.createDataFrame</code>. E.g. from a list of rows:</p><pre><code class="language-julia hljs">using Dates

df = spark.createDataFrame([
    Row(a=1, b=2.0, c=&quot;string1&quot;, d=Date(2000, 1, 1), e=DateTime(2000, 1, 1, 12, 0)),
    Row(a=2, b=3.0, c=&quot;string2&quot;, d=Date(2000, 2, 1), e=DateTime(2000, 1, 2, 12, 0)),
    Row(a=4, b=5.0, c=&quot;string3&quot;, d=Date(2000, 3, 1), e=DateTime(2000, 1, 3, 12, 0))
])
println(df)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">22/06/05 08:24:35 WARN SparkSession$Builder: Using an existing SparkSession; some spark core configurations may not take effect.
+---+---+-------+----------+-------------------+
|  a|  b|      c|         d|                  e|
+---+---+-------+----------+-------------------+
|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|
|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|
|  4|5.0|string3|2000-03-01|2000-01-03 12:00:00|
+---+---+-------+----------+-------------------+</code></pre><p>Or using an explicit schema:</p><pre><code class="language-julia hljs">df = spark.createDataFrame([
    [1, 2.0, &quot;string1&quot;, Date(2000, 1, 1), DateTime(2000, 1, 1, 12, 0)],
    [2, 3.0, &quot;string2&quot;, Date(2000, 2, 1), DateTime(2000, 1, 2, 12, 0)],
    [3, 4.0, &quot;string3&quot;, Date(2000, 3, 1), DateTime(2000, 1, 3, 12, 0)]
], &quot;a long, b double, c string, d date, e timestamp&quot;)
println(df)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">+---+---+-------+----------+-------------------+
|  a|  b|      c|         d|                  e|
+---+---+-------+----------+-------------------+
|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|
|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|
|  3|4.0|string3|2000-03-01|2000-01-03 12:00:00|
+---+---+-------+----------+-------------------+</code></pre><h2 id="Viewing-Data"><a class="docs-heading-anchor" href="#Viewing-Data">Viewing Data</a><a id="Viewing-Data-1"></a><a class="docs-heading-anchor-permalink" href="#Viewing-Data" title="Permalink"></a></h2><p>The top rows of a DataFrame can be displayed using <code>DataFrame.show()</code>:</p><pre><code class="language-julia hljs">df.show(1)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">+---+---+-------+----------+-------------------+
|  a|  b|      c|         d|                  e|
+---+---+-------+----------+-------------------+
|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|
+---+---+-------+----------+-------------------+
only showing top 1 row</code></pre><p>You can see the DataFrame’s schema and column names as follows:</p><pre><code class="language-julia hljs">df.columns()</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">5-element Vector{String}:
 &quot;a&quot;
 &quot;b&quot;
 &quot;c&quot;
 &quot;d&quot;
 &quot;e&quot;</code></pre><pre><code class="language-julia hljs">df.printSchema()</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">root
 |-- a: long (nullable = true)
 |-- b: double (nullable = true)
 |-- c: string (nullable = true)
 |-- d: date (nullable = true)
 |-- e: timestamp (nullable = true)</code></pre><p>Show the summary of the DataFrame</p><pre><code class="language-julia hljs">df.select(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;).describe().show()</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">+-------+---+---+-------+
|summary|  a|  b|      c|
+-------+---+---+-------+
|  count|  3|  3|      3|
|   mean|2.0|3.0|   null|
| stddev|1.0|1.0|   null|
|    min|  1|2.0|string1|
|    max|  3|4.0|string3|
+-------+---+---+-------+</code></pre><p><code>DataFrame.collect()</code> collects the distributed data to the driver side as the local data in Julia. Note that this can throw an out-of-memory error when the dataset is too large to fit in the driver side because it collects all the data from executors to the driver side.</p><pre><code class="language-julia hljs">df.collect()</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">3-element Vector{Row}:
 [1,2.0,string1,2000-01-01,2000-01-01 12:00:00.0]
 [2,3.0,string2,2000-02-01,2000-01-02 12:00:00.0]
 [3,4.0,string3,2000-03-01,2000-01-03 12:00:00.0]</code></pre><p>In order to avoid throwing an out-of-memory exception, use <code>take()</code> or <code>tail()</code>.</p><pre><code class="language-julia hljs">df.take(1)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">1-element Vector{Row}:
 [1,2.0,string1,2000-01-01,2000-01-01 12:00:00.0]</code></pre><h2 id="Selecting-and-Accessing-Data"><a class="docs-heading-anchor" href="#Selecting-and-Accessing-Data">Selecting and Accessing Data</a><a id="Selecting-and-Accessing-Data-1"></a><a class="docs-heading-anchor-permalink" href="#Selecting-and-Accessing-Data" title="Permalink"></a></h2><p>Spark.jl <code>DataFrame</code> is lazily evaluated and simply selecting a column does not trigger the computation but it returns a <code>Column</code> instance.</p><pre><code class="language-julia hljs">df.a</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">col(&quot;a&quot;)</code></pre><p>In fact, most of column-wise operations return <code>Column</code>s.</p><pre><code class="language-julia hljs">typeof(df.c) == typeof(df.c.upper()) == typeof(df.c.isNull())</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">true</code></pre><p>These <code>Column</code>s can be used to select the columns from a <code>DataFrame</code>. For example, <code>select()</code> takes the <code>Column</code> instances that returns another <code>DataFrame</code>.</p><pre><code class="language-julia hljs">df.select(df.c).show()</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">+-------+
|      c|
+-------+
|string1|
|string2|
|string3|
+-------+</code></pre><p>Assign new Column instance.</p><pre><code class="language-julia hljs">df.withColumn(&quot;upper_c&quot;, df.c.upper()).show()</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">+---+---+-------+----------+-------------------+-------+
|  a|  b|      c|         d|                  e|upper_c|
+---+---+-------+----------+-------------------+-------+
|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|STRING1|
|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|STRING2|
|  3|4.0|string3|2000-03-01|2000-01-03 12:00:00|STRING3|
+---+---+-------+----------+-------------------+-------+</code></pre><p>To select a subset of rows, use <code>filter()</code> (a.k.a. <code>where()</code>).</p><pre><code class="language-julia hljs">df.filter(df.a == 1).show()</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">+---+---+-------+----------+-------------------+
|  a|  b|      c|         d|                  e|
+---+---+-------+----------+-------------------+
|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|
+---+---+-------+----------+-------------------+</code></pre><h2 id="Grouping-Data"><a class="docs-heading-anchor" href="#Grouping-Data">Grouping Data</a><a id="Grouping-Data-1"></a><a class="docs-heading-anchor-permalink" href="#Grouping-Data" title="Permalink"></a></h2><p>Spark.jl <code>DataFrame</code> also provides a way of handling grouped data by using the common approach, split-apply-combine strategy. It groups the data by a certain condition applies a function to each group and then combines them back to the <code>DataFrame</code>.</p><pre><code class="language-julia hljs">df = spark.createDataFrame([
    [&quot;red&quot;, &quot;banana&quot;, 1, 10], [&quot;blue&quot;, &quot;banana&quot;, 2, 20], [&quot;red&quot;, &quot;carrot&quot;, 3, 30],
    [&quot;blue&quot;, &quot;grape&quot;, 4, 40], [&quot;red&quot;, &quot;carrot&quot;, 5, 50], [&quot;black&quot;, &quot;carrot&quot;, 6, 60],
    [&quot;red&quot;, &quot;banana&quot;, 7, 70], [&quot;red&quot;, &quot;grape&quot;, 8, 80]], [&quot;color string&quot;, &quot;fruit string&quot;, &quot;v1 long&quot;, &quot;v2 long&quot;])
df.show()</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">22/06/05 08:24:44 WARN SparkSession$Builder: Using an existing SparkSession; some spark core configurations may not take effect.
+-----+------+---+---+
|color| fruit| v1| v2|
+-----+------+---+---+
|  red|banana|  1| 10|
| blue|banana|  2| 20|
|  red|carrot|  3| 30|
| blue| grape|  4| 40|
|  red|carrot|  5| 50|
|black|carrot|  6| 60|
|  red|banana|  7| 70|
|  red| grape|  8| 80|
+-----+------+---+---+</code></pre><p>Grouping and then applying the <code>avg()</code> function to the resulting groups.</p><pre><code class="language-julia hljs">df.groupby(&quot;color&quot;).avg().show()</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">+-----+-------+-------+
|color|avg(v1)|avg(v2)|
+-----+-------+-------+
|  red|    4.8|   48.0|
|black|    6.0|   60.0|
| blue|    3.0|   30.0|
+-----+-------+-------+</code></pre><h2 id="Getting-Data-in/out"><a class="docs-heading-anchor" href="#Getting-Data-in/out">Getting Data in/out</a><a id="Getting-Data-in/out-1"></a><a class="docs-heading-anchor-permalink" href="#Getting-Data-in/out" title="Permalink"></a></h2><p>Spark.jl can read and write a variety of data formats. Here&#39;s a few examples.</p><h3 id="CSV"><a class="docs-heading-anchor" href="#CSV">CSV</a><a id="CSV-1"></a><a class="docs-heading-anchor-permalink" href="#CSV" title="Permalink"></a></h3><pre><code class="language-julia hljs">df.write.option(&quot;header&quot;, true).csv(&quot;data/fruits.csv&quot;)
spark.read.option(&quot;header&quot;, true).csv(&quot;data/fruits.csv&quot;)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi"></code></pre><h3 id="Parquet"><a class="docs-heading-anchor" href="#Parquet">Parquet</a><a id="Parquet-1"></a><a class="docs-heading-anchor-permalink" href="#Parquet" title="Permalink"></a></h3><pre><code class="language-julia hljs">df.write.parquet(&quot;data/fruits.parquet&quot;)
spark.read.parquet(&quot;data/fruits.parquet&quot;)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi"></code></pre><h3 id="ORC"><a class="docs-heading-anchor" href="#ORC">ORC</a><a id="ORC-1"></a><a class="docs-heading-anchor-permalink" href="#ORC" title="Permalink"></a></h3><pre><code class="language-julia hljs">df.write.orc(&quot;data/fruits.orc&quot;)
spark.read.orc(&quot;data/fruits.orc&quot;)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi"></code></pre><h2 id="Working-with-SQL"><a class="docs-heading-anchor" href="#Working-with-SQL">Working with SQL</a><a id="Working-with-SQL-1"></a><a class="docs-heading-anchor-permalink" href="#Working-with-SQL" title="Permalink"></a></h2><p><code>DataFrame</code> and Spark SQL share the same execution engine so they can be interchangeably used seamlessly. For example, you can register the <code>DataFrame</code> as a table and run a SQL easily as below:</p><pre><code class="language-julia hljs">df.createOrReplaceTempView(&quot;tableA&quot;)
spark.sql(&quot;SELECT count(*) from tableA&quot;).show()</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">+--------+
|count(1)|
+--------+
|       8|
+--------+</code></pre><pre><code class="language-julia hljs">spark.sql(&quot;SELECT fruit, sum(v1) as s FROM tableA GROUP BY fruit ORDER BY s&quot;).show()</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">+------+---+
| fruit|  s|
+------+---+
|banana| 10|
| grape| 12|
|carrot| 14|
+------+---+</code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../">« Introduction</a><a class="docs-footer-nextpage" href="../streaming/">Structured Streaming »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.18 on <span class="colophon-date" title="Sunday 5 June 2022 08:25">Sunday 5 June 2022</span>. Using Julia version 1.7.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
